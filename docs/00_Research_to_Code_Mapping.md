# Research to Code Mapping

This document provides a structured mapping between the theoretical concepts presented in the QuantArt research paper and their concrete implementation in the codebase.

| Paper Concept               | Notation (Approx.)                                                   | Code Implementation       | Location                                   | Notes/Discrepancies                                                                                               |
| :-------------------------- | :------------------------------------------------------------------- | :------------------------ | :----------------------------------------- | :---------------------------------------------------------------------------------------------------------------- |
| **Vector Quantized VAE**    | $E, G, Z, \mathcal{C}$                                               | `ExperimentStage2`        | `quantart/models/stage_2.py`               | The main model coordinating style transfer. Base VQGAN is `ExperimentStage1` in `quantart/models/stage_1.py`.     |
| **Encoder (Content)**       | $E_c(x)$                                                             | `self.encoder`            | `quantart/models/stage_2.py:41`            | Standard VQGAN encoder.                                                                                           |
| **Encoder (Style/Ref)**     | $E_s(y)$                                                             | `self.encoder_real`       | `quantart/models/stage_2.py:42`            | Used to encode the reference image.                                                                               |
| **Quantization**            | $q(z)$                                                               | `VectorQuantizer`         | `quantart/components/quantizer.py`         | Euclidean distance based quantization with codebook.                                                              |
| **Codebook**                | $\mathcal{C} = \{e_k\}_{k=1}^K$                                      | `self.embedding`          | `quantart/components/quantizer.py:42`      | Learnable embedding layer.                                                                                        |
| **Style Transfer Network**  | $T(z_c, z_s)$                                                        | `StyleTransferModule`     | `quantart/components/style_transfer.py:30` | Maps quantized content and style codes to target codes.                                                           |
| **Decoder/Generator**       | $G(z)$                                                               | `self.decoder`            | `quantart/models/stage_2.py:43`            | Decodes quantized tokens back to image space.                                                                     |
| **Codebook Loss**           | $\mathcal{L}_{VQ} = \|sg[z_e] - e\|_2^2 + \beta \|z_e - sg[e]\|_2^2$ | `VectorQuantizer.forward` | `quantart/components/quantizer.py:105`     | `beta` controls commitment loss weight. Supports legacy mode.                                                     |
| **Reconstruction Loss**     | $\mathcal{L}_{rec}$                                                  | `calc_content_loss`       | `quantart/losses/stage_2.py:68`            | Implemented as MSE loss (content loss).                                                                           |
| **Style Loss**              | $\mathcal{L}_{style}$                                                | `calc_style_loss`         | `quantart/losses/stage_2.py:73`            | Matching mean and std deviation of features (AdaIN-like statistic matching).                                      |
| **Adversarial Loss**        | $\mathcal{L}_{adv}$                                                  | `Stage2Loss`              | `quantart/losses/stage_2.py`               | PatchGAN discriminator (`NLayerDiscriminator`) with hinge or vanilla loss.                                        |
| **Perceptual Loss (LPIPS)** | $\mathcal{L}_{LPIPS}$                                                | `LPIPS`                   | `quantart/losses/lpips.py`                 | The `Stage2Loss` imports `LPIPS` but currently focuses on GAN+Style+Content loss. LPIPS might be used in Stage 1. |

## Key Discrepancies & Implementation Details

1.  **Latent Space Operation**: The Style Transfer Module operates entirely within the quantized latent space of a pre-trained VQGAN (`checkpoint_encoder`/`decoder` loaded in `ExperimentStage2`). This allows for efficiency but constrains the generation to the VQGAN's capabilities.
2.  **Style Loss Implementation**: The style loss in `Stage2Loss` is a simple mean/std matching (reminiscent of AdaIN) on the input/output tensors.
3.  **Quantization Bug**: The `VectorQuantizer` class has a comment about a bug regarding `beta` application (lines 30-32). The code supports a `legacy=True` flag (default) to maintain backward compatibility with old Taming Transformers checkpoints.
